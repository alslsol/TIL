# 5. 머신러닝

생성 일시: 2025년 2월 27일 오전 9:43
최종 편집 일시: 2025년 3월 5일 오후 5:37

# 1. 개론

- 약인공지능: 인간이 처리하기 복잡한 계산/노동 대신 처리하는 것 → 현재 GPT 등
    
    강인공지능(AGI): 사실상 사람과 똑같이 인지, 판단 내리는 것
    
- 데이터 회귀 분석 연구 나오며 인공지능 붐 일음
    
    회귀 분석: 수치들이 어떤  연관성 있는지 밝히는 것 → 이를 다룬 퍼셉트론 연구는 로지스틱 선형 회귀로 나아감
    
- **트랜스포머 모델 ⊂ 딥러닝 ⊂ 머신러닝 ⊂ 인공지능**
    - 인간 지능 모방하는 게 인공지능의 메인 카테고리
    - 머신러닝(기계학습): 기계에게 학습시키는 것 → 엄청나게 많은 데이터 밀어넣어 학습 시켜 판단결정 내리도록 만들기 위함
        - 수업: 사이킷런 라이브러리 사용할 것
    - 딥러닝: 학습 깊이/양 많아진 셈 → 인공지능 정확도 높아지게 됨
        - 수업: 텐서폴로 배울 것
- 2017년 이후 트랜스포머 모델 나와 주류 형성
    - 트랜스포머 이전:  LSTM 사용 → ex. 아이폰 자동 완성 기능
        
        문제: 전체 문장 맥락X, 바로 앞 단어와의 연관성만 파악해 정확도 떨어짐
        
    - 트랜스포머: 앞선 모든 단어를 참고하는 attention 활용 → 정확도 높음
        
        전체 맥락 고려해야 하는 번역 연구에서 출발 → 이미지/음악 등으로 확대 적용
        
    
- 머신러닝: 도미에 대한 데이터를 줌 → 그러면 알아서 수치간 연관성 파악해, 어떤 수치가 도미인지 결정하는데 결정적인 역할하는지 스스로 판단함
    - df의 ‘컬럼(열)’ = 머신러닝의 ‘특성’

# 2. 분류

### 실습: 생선 분류하기(먼 길)

1. 전처리: Fish df를 list로 전환하기
    
    ```python
    bream_df = df.loc[df['Species'] == 'Bream'] # 도미만 뽑기
    
    # length2, weight만 추출: df를 Series 형태로 바꿈 >> tolist() 통해 list로 전환
    bream_lenght = bream_df['Length2'].tolist()
    bream_weight = bream_df['Weight'].tolist()
    ```
    
- 도미 특성 산포도 그리기: matplotlib 라이브러리 활용
    - `plt.graph_name(x축=list1, y축=list2)`
        - scatter: 산포도
        - plot: 선 그래프
        - bar: 막대 그래프
    
    ```python
    import matplotlib.pyplot as plt # matplotlib 라이브러리 중 pyplot 기능 쓰기
    plt.scatter(bream_lenght, bream_weight)
    ```
    
- 빙어 고르기 → 길이/넓이 구하기
    
    ```python
    smelt_df = df[df['Species'] == 'Smelt']
    smelt_length = smelt_df['Length2'].tolist()
    smelt_weight = smelt_df['Weight'].tolist()
    ```
    
- 도미/빙어 산포도 비교하기
    
    ```python
    plt.scatter(bream_length, bream_weight)
    plt.scatter(smelt_length, smelt_weight)
    	# x축: 모든 행 중, 0번째 열 선택 >> 꽃 길이 선택한 것
    	# y축: 모든 행 중, 1번째 열 선택 >> 꽃 넓이 선택한 것
    ```
    
1. 정답지 만들기(라벨링)

```python
# 길이/무게 데이터 하나로 합치기
length = bream_length + smelt_length
weight = bream_weight + smelt_weight
```

```python
# 사이킷런 라이브러리: 2차원 데이터 넣어야 함(like df)
# {{도미1: 길이1, 무게1}, {도미2: 길이2, 무게2}...} >> 이런 식으로 만들어야 하는 것
fish_data = [[l, w] for l, w in zip(length, weight)]
```

```python
# 정답 데이터 만들기: 뭐가 도미의 길이인지 알려주는 정답 데이터 만들어야 학습시킬 수 있음 >> 0/1 이진 분류할 것
    # 도미=1, 빙어=0으로 부를 것
fish_target = [1] * len(bream_weight) + [0] * len(smelt_weight) # 도미/빙어 무게에 각각 도미는 1, 빙어는 0 정답 라벨링
```

1. 학습시키기
- 클래스 가져오기 → 인스턴스해 사용 → 인공지능 학습시키기
    - KNeighborsClassifier 클래스 가져오기: 설정한 갯수만큼 인접 데이터 파악해 분류해줌
    
    ```python
    from sklearn.neighbors import KNeighborsClassifier # 클래스 가져오기
    kn = KNeighborsClassifier() # 인스턴스화
    
    kn.fit(fish_data, fish_target) # 학습지-정답 비교하며 학습
    kn.score(fish_data, fish_target) # 문제 푼 것 채점 >> 정답률 도출
    > 1.0 # 100점이란 의미
    
    # 길이 30, 무게 600인 생선은 무슨 종일지 예측하란 것 >> 이때 리스트는 2차원으로 넣어줘야 함(대괄호 2개)
    kn.predict([[30, 600]])
    # > array([1]): 도미라고 답한 것
    ```
    
- 예측하기: 학습한 데이터 기반으로 예측함 → 학습 데이터와 가장 가까운 데이터를 찾아 정답을 도출하는 것
    - 만약 모든 데이터를 바라보도록 지정한다면
    - # > 49개의 데이터를 바라보게 되니, 점 어디를 찍어도 갯수가 더 많은 도미를 답으로 내놓을 확률이 더 높아져서, 정답 맞추는 확률 더 낮아진 것
        # 하이퍼 파라미터: 가장 성능 좋은 파라미터를 알아내서 사람이 지정해줘야 함

→ 근데 지금까지는, 정답지 펴 놓고 문제 풀으라 한 셈

훈련 세트, 테스트 세트로 데이터 분리해서, 훈련/검증 세트 따로 설정해야 함

## 2.1 지도/비지도 학습

- 지도 학습: 정답지 펴놓고 분류하는 방식
    - 훈련 세트: 훈련할 때 사용하는 데이터 세트 → 훈련 데이터 + 정답 데이터 세트
    - 테스트 세트: 평가할 때 사용하는 데이터 세트 → 데스트 데이터 + 정답 데이터 세트
    
    → 총 4개의 데이터 세트 필요
    
- 비지도 학습: 정답 알려주지 않고 카테고리 분류하는 방식

## 2.2 K-최근접 이웃(**KNeighbors)**

- **KNeighbors 알고리즘 원리**
    - input 받은 데이터와 가장 근접한 인근의 5개(기본값) 데이터와 비교해 예측

### 실습: 생선 분류하기(지름길)

1. 전처리: df → list 변환(위 과정 압축)
- `isin()` 활용: 2개 이상 데이터의 유무 알려줌 → 한번에 도미/빙어 추출 가능
    
    `values.tolist()`: df를 list로 변경해줌
    

```python
# 훈련 데이터 생성
df = pd.read_csv('data/Fish.csv')
cond = df['Species'].isin(['Bream', 'Smelt']) # 도미/빙어 한번에 필터링
lw_df = df.loc[cond, ['Length2', 'Weight']] # 길이/무게 컬럼만 가져옴
fish_data = lw_df.values.tolist() # df를 list로 전환

# 답지 생성
fish_target = [1] * 35 + [0] * 14
```

1. 데이터 섞은 뒤 → train/test 데이터 세트 input/target 총 4개 만들기
- numpy의 `random.shuffle` 활용: 데이터 무작위로 섞기

```python
import numpy as np
# 리스트를 넘파이 array 2차원 형태로 만듦
input_arr = np.array(fish_data)
target_arr = np.array(fish_target)

np.random.seed(42)
	# seed 번호 부여 시 한번 셔플한 값 고정됨 >> 안 그럼 print 할 때마다 숫자 달라짐
index = np.arange(49) 
np.random.shuffle(index) # 0-48까지의 숫자를 무작위 섞는 것
print(index)

# 무작위 섞은 숫자 활용해서 훈련 데이터 세트 만들 것
train_input = input_arr[index[:35]] #35번째까지 넣음
# 한번 섞은 다음 일부만 훈련 데이트 세트로 쓰기 위함
train_target = target_arr[index[:35]] # 한번 섞은 다음, 35번째까지 훈련용 정답지 만든 것

# 트레인 데이터 셋도 똑같이 만들기
test_input = input_arr[index[35:]]
test_target = target_arr[index[35:]]

# 훈련/테스트 학습 데이터 잘 섞인 것 확인 가능
import matplotlib.pyplot as plt
plt.scatter(train_input[:, 0], train_input[:, 1]) # 트레인 인풋: 길이/무게 같이 들어있는 리스트임 >> 전체 데이터 중 0번째가 길이데이터라고 지정한 것
plt.scatter(test_input[:, 0], test_input[:, 1])
```

1. 학습시키기

```python
kn.fit(train_input, train_target)
	# 35개 데이터 학습, 나머지 14개로 테스트하기
kn.score(test_input, test_target)
kn.predict(test_input) # 어떻게 예측했나 확인
```

### 실습: iris 분류하기

- setosa, virginica 분류: petal_length, petal_width 기준으로 분류
1. 전처리

```python
# seaborn 설치한 뒤 iris 데이터 불러오기
import seaborn as sns
df = sns.load_dataset('iris')
df.head()
# iris(붓꽃) 데이터: 꽃의 특성 대한 길이/너비 등 정보 있음 >> 이를 갖고 세부 종 구분돼 있음

# 학습 데이터 생성
cond_df = df[df['species'].isin(['setosa', 'virginica'])]
iris_data = cond_df[['petal_length', 'petal_width']].values.tolist()
cond_df['species'].value_counts()
iris_data

# setosa = 1, virginica = 0 부여해 답지 생성
iris_target = [1] * 50 + [0] * 50
iris_target
```

1. 데이터 무작위 섞기

```python
input_arr = np.array(iris_data)
target_arr = np.array(iris_target)

np.random.seed(42)
index = np.arange(99)
np.random.shuffle(index)

train_input = input_arr[index[:50]]
train_target = target_arr[index[:50]]

test_input = input_arr[index[50:]]
test_target = target_arr[index[50:]]

# 산포도로 섞인 거 확인
plt.scatter(train_input[:, 0], train_input[:,1])
	# x축: 모든 행 중, 0번째 열 선택 >> 꽃 길이 선택한 것
	# y축: 모든 행 중, 1번째 열 선택 >> 꽃 넓이 선택한 것
plt.scatter(test_input[:, 0], test_input[:, 1])
```

1. 학습시키기

```python
kn.fit(train_input, train_target)
kn.score(test_input, test_target)
kn.predict(test_input)
```

### 실습: iris 또다른 방법

```python
import seaborn as sns
iris = sns.load_dataset('iris')
iris.head()

cond = iris['species'].isin(['setosa', 'versicolor'])
iris_df = iris.loc[cond, ['petal_length', 'petal_width']]
iris_data = iris_df.values

# 종류별 갯수 세기
setosa_count = iris.loc[iris['species'] == 'setosa'].shape[0]
	# 세토사만 뽑기 >> shape으로 행/열 갯수 튜플로 뽑기 >> 그중 행 갯수만 추출 위해 [0]
versicolor_count = iris.loc[iris['species'] == 'versicolor'].shape[0] #버지컬러 행 뽑기

# 정답지 만들기: 1/0 할당하기 >> 그럼 1, 0으로 가득찬 리스트 만들어짐 >> 리스트 1, 리스트 0으로 가득 차게 됨
import numpy as np
iris_target = np.array([1] * setosa_count + [0] * versicolor_count)
    # 이후 팬시 인덱싱 위해 넘파이 형태로 변환

# 무작위로 섞기: 파이썬 range() 활용
import random
index = list(range(setosa_count + versicolor_count)) # 데이터 총합 갯수 세기 >> 이후 총합 갯수만큼의 수 있는 list 만들기
random.shuffle(index) # random 모듈 활용해서 데이터 총합만큼 있는 숫자 리스트 무작위 섞기

# 맨앞~70까지 학습, 70~맨끝 30개는 테스트하기
train_input = iris_data[index[:70]] # 팬시 인덱싱 하는 것 >> 그러려면 넘파이이거나 df 형태여야 함
train_target = iris_target[index[:70]]

test_input = iris_data[index[70:]]
test_target = iris_target[index[70:]]

# 산포도 그리기
plt.scatter(train_input[:, 0], train_input[:, 1]) # 이거 왜하는 거임??
plt.scatter(test_input[:, 0], test_input[:, 1])
```

## 2.3 데이터 전처리

- `train_test_split(data_name, target_name)`: 입력받은 리스트/배열을 훈련/데이터 세트로 알아서 나눠줌
    - 4개 리스트 연달아 나옴: 순서대로 input(train → test) → target(tarin → test)
    - 인덱스 무작위로 섞고 >> 비율대로 train/test 구분해줌 >> 리스트로 나온 것 4등분해야 함
    - 기본값: 전체 데이터 100개면 25%는 test, 75%는 훈련 데이터로 사용
    
    ```python
    import pandas as pd
    df = pd.read_csv('data/Fish.csv')
    
    cond = df['Species'].isin(['Bream', 'Smelt'])
    fish_data = df.loc[cond, ['Length2', 'Weight']]
    
    # 0/1 대신 이름 자체를 활용해 전처리하기
    fish_target = df.loc[cond, ['Species']]
    
    from sklearn.model_selection import train_test_Fsplit # 사이킷런의 해당 함수 가져오기
    
    train_input, test_input, train_target, test_target = \
    	train_test_split(fish_data, fish_target)
    # 각각 리스트로 묶인 것을 순서대로 객체에 할당하겠단 것임
    ```
    
- 학습시키기
    
    ```python
    from sklearn.neighbors import KNeighborsClassifier
    kn = KNeighborsClassifier()
    
    kn.fit(train_input, train_target['Species'])
    kn.score(test_input, test_target['Species']) 
    ```
    
- 산포도 그리기
    
    ```python
    import matplotlib.pyplot as plt
    
    plt.scatter(train_input['Length2'], train_input['Weight'])
    plt.scatter(25, 150, marker = '^') # 마커 옵션: 산점도의 점 모양 변경
    ```
    
- 표준점수 활용하기: 데이터/target 모두 표준점수로 변경해야 함
    - 표준점수 생성
    
    ```python
    # 평균, 표준편차 구하기
    mean = train_input.mean()
    std = train_input.std()
    
    train_scaled = (train_input - mean) / std # 각 값에서 평균 빼고, std로 나눈 값이 나옴 >> 그럼 표준점수로 변환되는 것
        # 스케일이 일정히 바뀐 데이터인 것
    
    plt.scatter(train_scaled['Length2'], train_scaled['Weight'])
    # x, y축 변경돼 표시됨 >> 그럼 다른 자료와 비교 더 명확히 할 수 있음
    ```
    
    - 학습시키기
    
    ```python
    kn.fit(train_scaled, train_target['Species']) # 표준점수로 학습시키기
    
    test_scaled = (test_input - mean) / std
    kn.score(test_scaled, test_target['Species']) # 평가하는 데이터도 표준점수화 시켜야 함
    ```
    
    - 입력값 예측하기
    
    ```python
    new = ([25, 150] - mean) / std
    kn.predict([new])
    ```
    

### 연습문제: Adelie와 Gentoo 구분하기

### 코드1

```python
1. 데이터 불러오기
import seaborn as sns
df = sns.load_dataset('penguins')
df.notna().sum()
cond = df['species'].isin(['Adelie', 'Gentoo'])
	# 컬럼 선택한 뒤, 해당 컬럼에 이런 범주 있는지 묻고 필터링
df_copy = df[cond].dropna()
df_copy.isnull().sum()

2. 효과적으로 구분할 수 있는 특성 고르기(산포도) 필요
    # bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g
target = df_copy.loc[cond, ['species']]

# 학습 데이터 생성
data = df_copy.loc[cond, ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']].values.tolist()

# target 전처리하기 
train_input, test_input, train_target, test_target = train_test_split(data, target)
     # 잘 구분됐는지 확인하는 법: list 갯수는 len()으로 셀 수 있음
         # len(train_input) # 207개
         # len(test_input) # 69개
         
3. 가장 차이 뚜렷한 것 활용하기
data = df_copy.loc[cond, ['bill_depth_mm', 'flipper_length_mm']].values.tolist()
target = df_copy.loc[cond, ['species']]
train_input, test_input, train_target, test_target = train_test_split(data, target)

train_input_np = np.array(train_input)
test_input_np = np.array(test_input)

plt.scatter(train_input_np[:, 0], train_input_np[:, 1])
plt.scatter(test_input_np[:, 0], test_input_np[:, 1])

4. 학습
from sklearn.neighbors import KNeighborsClassifier # 클래스 가져오기
kn = KNeighborsClassifier()
kn.fit(train_input, train_target['species'])
kn.score(test_input, test_target)
```

### 코드2(강사님)

```python
import pandas as pd
import seaborn as sns
df = sns.load_dataset('penguins')

# 결측치 없애기: 데이터 300개 중 결측값 10개 정도로 적으니까 일괄 삭제
df = df.dropna() # df.dropna(inplace=True) 와 동일 코드

# 아델리, 젠투 필터링
cond = df['species'].isin(['Adelie', 'Gentoo'])

# K-NN: 두 수치 극명히 나뉠 때 유의미 -> bill_depth_mm, body_mass_g만 사용
pg_data = df.loc[cond, ['bill_depth_mm', 'body_mass_g']]
pg_target = df.loc[cond, ['species']]

# 시각화하기
import matplotlib.pyplot as plt
plt.scatter(df.loc[df['species'] == 'Adelie', 'bill_depth_mm'], df.loc[df['species'] == 'Adelie', 'body_mass_g'])
plt.scatter(df.loc[df['species'] == 'Gentoo', 'bill_depth_mm'], df.loc[df['species'] == 'Gentoo', 'body_mass_g'])

# 학습 위해 4개 객체 만들기
from sklearn.model_selection import train_test_split
train_input, test_input, train_target, test_target =\
	train_test_split(pg_data, pg_target)

# 비교 위해 표준점수 만들기
mean = train_input.mean()
std = train_input.std()
train_scaled = (train_input - mean) / std
test_scaled = (test_input - mean) / std

# 학습
from sklearn.neighbors import KNeighborsClassifier
kn = KNeighborsClassifier()
kn.fit(train_scaled, train_target)
kn.score(test_scaled, test_target)
```

# 3. 회귀 알고리즘

- 회귀: 두 변수 상관관계 분석해 수치 예측하기 → 수치 예측하는 것
    
    ex. 길이, 높이, 두께 수치 데이터 3개를 토대로 무게 수치 데이터 예측하기
    
    - k-최근접 이웃 **분류**: 입력값과 인접한 데이터 확인 → 다수결 원칙으로 이웃 데이터의 비율로 예측
    - k-최근접 이웃 **회귀**: 이웃의 n개 데이터의 평균 토대로, 입력값 예측
- 예제: k-최근접 회귀 알고리즘
    - 새로 들어온 도미의 경우 무게 얼마일지 예측하기
    - 예측 원리: 결정계수(R**²**) → 타깃값, 예측or평균값 차이 얼마나 적는지 계산함
    
    ```python
    import pandas as pd
    df = pd.read_csv('data/Fish.csv')
    
    # 도미 데이터만 필터링
    cond = df['Species'] == 'Perch'
    perch_data = df.loc[cond]
    
    # 길이-몸무게 상관관계 그래프 그려 상관관계 확인하기
    import matplotlib.pyplot as plt
    plt.scatter(perch_data['Length2'], perch_data['Weight'])
        # 길이 증가할수록 무게 증가함 확인
    
    # 전처리
    from sklearn.model_selection import train_test_split
    train_input, test_input, train_target, test_target = \
    	train_test_split(perch_data[['Length2']], perch_data[['Weight']])
    
    # 학습시키기
    from sklearn.neighbors import KNeighborsRegressor # Regressor 도구 불러오기
    knr = KNeighborsRegressor() # 인스턴스
    knr.fit(train_input, train_target)
    knr.score(test_input, test_target) # 정확도
    ```
    
- 학습 잘 된 데이터: 훈련셋은 90점 중후반, 테스트셋은 그보단 조금 낮아야 함
    
    → 아닐 시, 과대적합/과소적합 문제 발생할 수 있음
    
    - 하이퍼 파라미터: 최적합 파라미터 사람이 찾아서 최적화 모델 만들어야 함
        - 몇 개의 이웃과 비교할 것인지 파라미터 값 수정하며 최적의 값 찾기
        - 예시: 하이퍼 파라미터 찾기
        
        ```python
        knr.n_neighbors = 3 # 기본값에서 3으로 변경
        knr.fit(train_input, train_target)
        print(knr.score(train_input, train_target))
        print(knr.score(test_input, test_target))
        ```
        

## 3.1 선형 회귀(`LinearRegression`)

- from `sklearn.linear_model` import `LinearRegression`
- k-최근접 알고리즘 문제: 이웃 비교하기에, 극단치 입력 시 예측 정확도 떨어짐
    
    → 가상의 선 그려서 극단치 입력받아도 예측하도록 선형 회귀 분석 하는 것
    
- 선형 회귀: 개별 데이터 가장 잘 나타내는 회귀선 그림 → 회귀선 토대로 수치 예측
- 예제: 도미 길이에 따른 무게 예측하기
    - 선형 회귀 알고리즘: `from sklearn.linear_model import LinearRegression`
    - `train_test_split(예측 기준으로 삼을 값, 예측할 값)`
    - `fit` 단계에서 2차원 아니어서 오류날 때: 2번째 단계에서 대괄호 2개 넣어 2차원 데이터로 만들어야 함
    - k-최근접: 무게 증가/감소할 때 최대/최소값 그대로
        
        ↔ 선형회귀분석: 회귀선 기반으로 값 예측 → 학습 데이터에 없는 값도 예측O
        
    
    ```python
    # 데이터 불러오기
    import pandas as pd
    df = pd.read_csv('data/Fish.csv')
    data = df.loc[df['Species'].isin(['Bream', 'Smelt']), ['Weight', 'Length2', 'Height']]
    	# 도미/빙어 있다면 필터링 -> 그중 3개 컬럼만 선택
    target = df.loc[df['Species'].isin(['Bream', 'Smelt']), 'Species'].map({'Bream': 0, 'Smelt': 1})
    	# 도미/빙어 있다면 필터링 -> 그중 Species 컬럼만 선택 -> Bream은 0, Smelt는 1로 교체
        # 선형 회귀는 target이 수치 값이어야 계산 가능 -> string 범주형이면X
        
    # 셋 나누기
    from sklearn.model_selection import train_test_split
    train_input, test_input, train_target, test_target = train_test_split(data, target)
    	# 3개 컬럼과 값 들어 있는 data 기반으로, 각 값이 어떤 생선인지 맞추겠단 것(target)
    
    # 정규화
    from sklearn.preprocessing import StandardScaler
    ss = StandardScaler()
    ss.fit(train_input, train_target)
    train_scaled = ss.transform(train_input)
    test_scaled = ss.transform(test_input)
    
    # 학습
    from sklearn.linear_model import LinearRegression # 선형회귀 클래스 선언
    lr = LinearRegression() # 인스턴스화
    lr.fit(train_scaled, train_target)
    print(lr.score(train_scaled, train_target))
    print(lr.score(test_scaled, test_target))
    ```
    
    ```python
    1. 데이터 불러오기
    import pandas as pd
    df = pd.read_csv('data/Fish.csv')
    
    2. 도미 필터링 -> 길이/무게만 추출
    perch_df = df.loc[df['Species'] == 'Perch']
    perch_length = perch_df[['Length2']]
    perch_weight = perch_df[['Weight']]
    
    3. 학습 데이터 만들기
    from sklearn.model_selection import train_test_split
    train_input, test_input, train_target, test_target =\
     train_test_split(perch_length, perch_weight)
        # 길이 기반으로 무게 찾을 거니까 이렇게 작성
    
    4. 선형 회귀 알고리즘 가져오기: linear
    from sklearn.linear_model import LinearRegression
    lr = LinearRegression() # 인스턴스화
    lr.fit(train_input, train_target)
    lr.score(test_input, test_target)
    
    5. 예측하기: 50cm 도미 넣으면 무게는?
    lr.predict([[100]])
    ```
    
- `lr.coef_`: 가중치(기울기) → a클수록 x에 미치는 영향 커짐
    
    `lr.intercept_`: (y절편)
    
- 선형 회귀 모델 한계: 회귀선 일직선 + 음수까지 이어짐 → 음수 무게의 생선 예측하게 됨
    
    → 그래서 데이터에 적합한 형태 만들려 곡선형태의 다항 회귀 사용
    
    ```python
    lr.predict([[1]]) # 1cm 도미 무게 입력했는데, 음수값 나옴
    >array([[-626.20587197]])
    ```
    

## 3.2 다항 회귀

- 구조: ax² + bx + c
    - 길이 1개 값을 기준으로 → 이를 제곱하는 등 활용해 예측
- 예제: 다항 회귀 분석으로 생선 길이에 따른 무게 구하기
    - 핵심: x**²**값 구하기 위해 x인 길이 제곱한 뒤 → 새 컬럼 생성해 할당
    - `lr.predict([[예측하고자 하는 길이, 그 제곱값]])`: 컬럼 추가해 2개 됐으니까 2개 데이터 넣음
    - `print(lr.coef_, lr.intercept_)`: [[bx, ax**²**]] [c] 순으로 출력됨
    
    ```python
    1. Lenght2 값 제곱해 새 컬럼으로 추가
    train_input['Length2_poly'] = train_input['Length2'] ** 2
    test_input['Length2_poly'] = test_input['Length2'] ** 2
    
    2. 학습
    lr = LinearRegression()
    lr.fit(train_input, train_target) # train_input: 길이, 길이 제곱값 들어 있음
    lr.score(test_input, test_target)
    lr.predict([[2, 4]]) # 컬럼 2개 됐으니까, 데이터 2개 넣어야 함
    print(lr.coef_, lr.intercept_)
    > [[-22.19684153   1.01734953]] [130.8814834]
    	# a=1, b=-22,ㄹ c=130 -> (x, x제곱) 순으로 넣었으니, 1번째가 x인 b의 값임
    ```
    

### 특성 공학: 다중 회귀

- 구조: ax + by + cz + d → 각각 x에 최적화 a값, y에 최적화 b값… 구하기
    - 2개 이상 서로 다른 특성을 활용해 예측하는 것
        
        ↔ 다항회귀: 1개 특성을 제곱해 활용
        
- 예제: 길이, 높이, 두께 3개 수치 토대로 예측하기
    - `from sklearn.preprocessing import PolynomialFeatures`: 내가 가진 함수를 변환해주는 역할
        - `PolynomialFeatures`: 특성공학 편리하게 해주는 기능
    - 특성공학 활용할 시, 지표 증가하므로 정답률 높아짐
        - 근데 특성 많이 생성한다고 다 좋은 건X, 최적화된 특성 몇 개만 만들어야 함
    
    ```python
    1. 데이터 불러오기
    import pandas as pd
    df = pd.read_csv('data/Fish.csv')
    perch_df = df.loc[df['Species'] == 'Perch']
    perch_df.head()
    
    2. Length2, Height, Width만 필터링 >> 훈련/테스트 세트 만들기
    perch_full = perch_df[['Length2', 'Height', 'Width']]
    perch_weight = perch_df[['Weight']]
    perch_weight.head()
    
    from sklearn.model_selection import train_test_split
    train_input, test_input, train_target, test_target = train_test_split(perch_full, perch_weight)
    
    3. 특성공학으로 poly 객체 만들기
    poly = PolynomialFeatures(include_bias=False)
    poly.fit(train_input)
    train_poly = poly.transform(train_input)
    	# 높이, 길이, 두께 3개 데이터 조합의 경우의 수를 만들어줌
    train_poly.shape # 생선 42마리에 대해 9개의 특성을 만들었음
    poly.get_feature_names_out() # 어떻게 연산한 9개의 특성을 만든 건지 확인
    
    4. 특성공학 바탕으로 학습시키기
    from sklearn.linear_model import LinearRegression
    lr = LinearRegression()
    lr.fit(train_poly, train_target) # 특성공학한 poly를 input 대신 넣어야 함
    lr.score(train_poly, train_target) # 학습 데이터 셋으로 정답률 확인하기
    
    # test 셋으로도 채점하기: test_poly도 만들어야 함
    test_poly = poly.transform(test_input)
    lr.score(test_poly, test_target)
    ```
    

### 규제

- 머신러닝 모델이 훈련 세트 너무 과도하게 학습하지 못하게 막는 것
    
    → 훈련 세트에 과대적합 방지 위함 | 기울기(가중치) 작게 만드는 것
    

### 릿지/라쏘

- 선형 회귀 모델에 규제를 추가한 모델
- **릿지(**`Ridge`**)**: 기울기 계수 제곱하는 방식
    - from `sklearn.linear_model` import `Ridge`
    
    **라쏘(**`Lasso`**)**: 기울기 계수 절댓값 통해 활용하는 방식
    
    → 각 방식 적용, 이후 계수 크기 줄여가며 x 대한 영향도 확인하는 것
    
    - `alpha` 옵션 통해 규제 강도 설정 가능
        - 하이퍼 알파: 규제 너무 강해도 훈련-테스트 셋 괴리 발생
            
            → 두 데이터 간 간극 최소화 할 수 있는 최적의 알파 찾아야 함
            
- 예제: 라쏘/릿지 통해 과대적합 해결하기
    
    ```python
    1. 특성 많은 경우 생성
    poly = PolynomialFeatures(degree=5, include_bias=False) # 몇개의 조합으로 만들지 대한 옵션(기본값은 2개) -> 높이의 5제곱 곱한 것 등 여러 특성을 만드는 것
    poly.fit(train_input)
    train_poly = poly.transform(train_input)
    test_poly = poly.transform(test_input)
    train_poly.shape
    # > (42, 55) -> 특성 55개 나온 것 -> 생선 데이터보다 특성 더 많아진 것 >> 이 경우 어떤지 학습시켜보기
    
    lr.fit(train_poly, train_target)
    print(lr.score(train_poly, train_target))
    print(lr.score(test_poly, test_target))
    # > 0.9999999997455769
    # -83.22115987225455
        # 테스트셋은 음수 나옴 -> 학습에 너무 최적화된 나머지, 채점해야 하는 데이터는 모두 틀린 것 -> 따라서 특성 많이 생성한다고 좋은 건 아님, 최적화된 특성 몇 개만 만드는 게 필요
        
    2. 데이터 정규화하기 -> 표준화하는 것
    from sklearn.preprocessing import StandardScaler
    ss = StandardScaler()
    ss.fit(train_poly)
    train_scaled = ss.transform(train_poly)
    test_scaled = ss.transform(test_poly)
    
    3-1. 릿지: 정규화된 데이터 바탕으로 규제하기
    from sklearn.linear_model import Ridge
    ridge = Ridge(alpha=0.1) # 규제의 강도 지정하는 옵션 줄 수 있음(기본값 1.0) >> 숫자 늘릴수록 규제 강하게 주겠단 것 -> 0.1일 때 성능 가장 좋음
    ridge.fit(train_scaled, train_target)
    print(ridge.score(train_scaled, train_target))
    print(ridge.score(test_scaled, test_target))
        # 특성 55개 짜리에 규제를 적용했더니 >> 안 쓸 데이터는 규제 넣어 0에 가깝게 간소화해서 정답률 정상적으로 나옴
    
    3-2. 랏소
    from sklearn.linear_model import Lasso
    lasso = Lasso()
    lasso.fit(train_scaled, train_target)
    print(lasso.score(train_scaled, train_target))
    print(lasso.score(test_scaled, test_target))
    ```
    

## 3.3 로지스틱 회귀

- from `sklearn.linear_model` import `LinearRegression`
 알고리즘 **한계**: 다수결 원칙으로 가중치 둬 정답 도출
    
    → 무게/길이 따라 두께 확률 계산하는 게X, 단순 주변에 뭐가 있는지가 중요
    
    - 예제: k-최근접 이웃 활용한 경우 → 정답률 성능 안 좋음
    
    ```python
    1. 데이터 불러오기
    import pandas as pd
    df = pd.read_csv('data/Fish.csv')
    df['Species'].unique() # 유니크한 값만 출력하기
    
    # 일부 컬럼만 추출
    fish_input = df[['Weight', 'Length2', 'Length3', 'Height', 'Width']]
    fish_input.head()
    	# 길이, 무게 등 데이터 봤을 때 이게 도미/숭어인지 맞추기 할 것
    
    fish_target = df[['Species']] # 인덱스에 따라 뭐가 숭어/도미인지 정답만 뺀 것 -> target은 정답(이름) 데이터만 가짐
    
    2. 훈련/테스트 셋 만들기
    from sklearn.model_selection import train_test_split
    train_input, test_input, train_target, test_target = train_test_split(fish_input, fish_target)
    
    3. 표준화하기: 무게, 길이 간 단위 등이 너무 차이나므로 -> 표준화해야 함
    from sklearn.preprocessing import StandardScaler
    ss = StandardScaler()
    ss.fit(train_input, train_target)
    train_scaled = ss.transform(train_input)
    test_scaled = ss.transform(test_input)
        # input만 표준점수로 바꾸는 것임 -> target은 str이니까
    
    4. 학습시키기
    from sklearn.neighbors import KNeighborsClassifier
    kn = KNeighborsClassifier()
    kn.fit(train_scaled, train_target)
    print(kn.score(train_scaled, train_target))
    print(kn.score(test_scaled, test_target))
    > 0.8571428571428571
    	0.75
    	# k-최근접 사용했을 때, 정합률 좋지 못함
    kn.predict_proba(test_scaled)
    ```
    
    - 예제: 도미/빙어 이진 분류
    
    ```python
    # 데이터 불러오기
    import pandas as pd
    df = pd.read_csv('data/Fish.csv')
    cond = df['Species'].isin(['Bream', 'Smelt'])
    data = df[cond].drop(columns='Species') # species만 삭제한 data 구조 만들기
    target = df.loc[cond, 'Species'].map({'Bream': 0, 'Smelt': 1})
        # 선형 회귀는 target이 수치 값이어야 계산 가능 -> string 범주형이면X
    # 셋 나누기
    from sklearn.model_selection import train_test_split
    train_input, test_input, train_target, test_target = train_test_split(data, target)
    
    # 정규화
    from sklearn.preprocessing import StandardScaler
    ss = StandardScaler()
    ss.fit(train_input, train_target)
    train_scaled = ss.transform(train_input)
    test_scaled = ss.transform(test_input)
    
    # 학습
    from sklearn.linear_model import LinearRegression
    lr = LinearRegression()
    lr.fit(train_scaled, train_target)
    print(lr.score(train_scaled, train_target))
    print(lr.score(test_scaled, test_target))
    ```
    
- 예제: 선형회귀 모델 + 릿지
    
    ```python
    # 데이터 불러오기
    import pandas as pd
    df = pd.read_csv('data/Fish.csv')
    cond = df['Species'].isin(['Bream', 'Smelt'])
    	# df의 Species 컬럼 중 일부 추출 -> df[df['Species']~] 이렇게 하면 cond 쓸 필요X
    data = df[cond].drop(columns='Species')
    target = df.loc[cond, 'Species'].map({'Bream': 0, 'Smelt': 1})
    
    # 셋 구분
    from sklearn.model_selection import train_test_split
    train_input, test_input, train_target, test_target = train_test_split(data, target)
    
    # 정규화
    from sklearn.preprocessing import StandardScaler
    ss = StandardScaler()
    ss.fit(train_input, train_target)
    train_scaled = ss.transform(train_input)
    test_scaled = ss.transform(test_input)
    
    # 릿지 모델
    from sklearn.linear_model import Ridge
    ridge = Ridge()
    ridge.fit(train_scaled, train_target)
    print(ridge.score(train_scaled, train_target))
    print(ridge.score(test_scaled, test_target))
    ```
    

### 개념 원리

- **로지스틱 선형 회귀**: 수치 예측하는 회귀X, 분류 모델 → 선형 방정식 찾는 게 핵심
    - 내가 가진 숫자 이용해 z값 도출
        
        → z값 클수록, 해당 데이터일 확률**↑** | -z일수록 해당 데이터일 확률**↓**
        
    - 시그모이드 함수에 넣으면 z 범위: 0 ≤ z ≤ 1 → 변환해 확률로 나타낼 수O
        - 시그모이드 함수 목표: 0~1로 z값 일정히 바꾸는 것
    
    ⇒ z값 수치 도출해 회귀는 맞으나, z값 통해 확률 계산이 핵심이라 분류 모델
    

### 이진 분류

- 이진 분류 예제: 도미/빙어 판단
    - 로지스틱 회귀 원리: 길이, 높이 등 데이터 기반으로 연산해 확률로 변환 → 확률 토대로 도미/빙어 판단
    
    ```python
    1. 이진 분류: 도미/빙어만 추출
    bream_smelt_indexes = (train_target == 'Bream') | (train_target == 'Smelt')
    train_bs = train_scaled[bream_smelt_indexes['Species']]
    	# train_scaled: [1, -1, - ...] 이런 식 >> 이중 true(1)만 고르기
    target_bs = train_target[bream_smelt_indexes['Species']]
        # train_scaled, train_target 모두 도미/빙어만 이진 분류로 필터링
    
    2. 로지스틱 회귀 모델 불러오기
    from sklearn.linear_model import LogisticRegression
    lr = LogisticRegression()
    lr.fit(train_bs, target_bs) # 이진분류한 것을 토대로 학습
    lr.predict(train_bs[:5]) # 이진분류된 학습 데이터를 5번째까지만 예측
    lr.predict_proba(train_bs[:5]).round(3)
    	# lr.predict_proba(): 도미/빙어로 판단한 구체적인 확률값 출력
        # round: numpy의 반올림하는 함수
    ```
    
- 예시

```python
# 데이터 불러오기
import pandas as pd
df = pd.read_csv('data/Fish.csv')
cond = df['Species'].isin(['Bream', 'Smelt'])
data = df[cond].drop(columns='Species') # species만 삭제한 data 구조 만들기
target = df.loc[cond, 'Species'].map({'Bream': 0, 'Smelt': 1})
    # 선형 회귀는 target이 수치 값이어야 계산 가능 -> string 범주형이면X

# 셋 나누기
from sklearn.model_selection import train_test_split
train_input, test_input, train_target, test_target = train_test_split(data, target)

# 정규화
from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
ss.fit(train_input, train_target)
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)

# 학습
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
lr.fit(train_scaled, train_target)
lr.score(test_scaled, test_target)
```

### 다중 분류

- 다중 분류 예제: 7마리 생선 모두 학습해 분류
    - 원리: 7마리 분류 위한 7개의 서로 다른 방정식 구해서 확률 계산
    
    ```python
    lr = LogisticRegression(C=20, max_iter=1000)
    	# 성능 높이기 위해 옵션 지정
        # max: 반복 횟수 정하는 것(기본값 100) -> 높을수록 성능 좋아짐
    lr.fit(train_scaled, train_target) # train_scaled: 7마리 모두 있음
    print(lr.score(train_scaled, train_target))
    print(lr.score(test_scaled, test_target))
    lr.predict_proba(test_scaled[:5]).round(3)
    	# 5번째까지 소수점 3번째까지 반올림해 확률 출력
    ```
    
    - 로지스틱 함수 활용 시 자동으로 `softmax()` 적용돼 1열 확률 총합 1로 해줌
        - 본래 로지스틱 함수 원리: 7개의 z값 계산해 각각 로지스틱 씌움 → 확률 총합 1 넘음
            
            → 그래서 softmax() 적용해야 확률 표준화해서 비교 가능
            
            `softmax()`: 데이터 여럿일 때 확률 총합 1 넘지 않게 해주는 역할
            
            → 딥러닝 파트에서 다룰 것
            
        - 로지스틱 회귀 = 작은 인공신경망과 비슷
            
            뉴런: 0/1로 변환함 → 로지스틱 회귀도 입력 받은 데이터 0/1 확률로 전환
            
    - 예제: 다중 분류 → 7마리 생선 모두 맞추기
    
    ```python
    # 데이터 불러오기
    import pandas as pd
    df = pd.read_csv('data/Fish.csv')
    data = df.drop(columns='Species')
    target = df[['Species']]
    
    # 셋 구분
    from sklearn.model_selection import train_test_split
    train_input, test_input, train_target, test_target = train_test_split(data, target)
    
    # 정규화
    from sklearn.preprocessing import StandardScaler
    ss = StandardScaler()
    ss.fit(train_input, train_target)
    train_scaled = ss.transform(train_input)
    test_scaled = ss.transform(test_input)
    
    # 학습
    from sklearn.linear_model import LogisticRegression
    lr = LogisticRegression()
    lr.fit(train_scaled, train_target)
    print(lr.score(train_scaled, train_target))
    print(lr.score(test_scaled, test_target))
    ```
    

### 확률적 경사 하강법(`SGDClassifier`)

- from `sklearn.linear_model` import `SGDClassifier`
- **점진적 학습**: 100의 데이터 20, 30, 50씩 조금씩 늘리며 학습시키는 것 → 이를 위해 SGD 활용
- **확률적 경사 하강법(SGD)**: 랜덤 갯수의 데이터 추출해 학습 → 손실 함수 최소화하기
    
    ex. 각 랜덤 갯수만큼 학습시킴 → 학습 데이터 산포도에 따라 → 편차(손실) 적은 방향으로 방정식 선 조정
    
- **손실 함수**(≒실패율): 머신러닝 알고리즘 얼마나 엉터리인지 측정하는 기준 → 작을수록 긍정적
    - 미분: 점의 기울기 찾는 것 | 미분 값=0 → 손실 함수 가장 적음
        
        미분 가능해야 해서 연속 데이터 필요 → 불연속적인 개수X, 연속적 확률인 로지스틱 함수의 예측값 사용
        
    - ex. 예측값(0.9) * 정답(1) = -0.9
        
        → 예측값 높아질수록 정답과 근접, 손실률 하락 → 이를 표현하기 위해 마이너스 붙임
        
        ex. 정답이 0일 때: 예측값(0.2), 정답(0) == 예측값(0.8), 정답(1) → 정답 1로 바꿔 계산할 것
        
    - 로그 씌우면 0/1에 급격히 가깝게 조정해줌
        
        정답 1인데, 확률 작은 경우 대해(잘못 예측한 경우), 강하게 패널티 줘 상대적 차이 강조
        
- 예제
    - 에포크 무한히 늘린다고 고성능X → 일정 수준 넘으면 일정한 값 나오는 지점 있음 → 그런 최적의 값 찾아야 함
    
    ```python
    1. 데이터 불러오기
    import pandas as pd
    df = pd.read_csv('data/Fish.csv')
    
    fish_input = df[['Weight', 'Length2', 'Length3', 'Height', 'Width']]
    fish_target = df[['Species']]
    
    2. 훈련/테스트 셋 구분하기
    from sklearn.model_selection import train_test_split
    train_input, test_input, train_target, test_target = train_test_split(fish_input, fish_target)
    
    3. 정규화하기
    from sklearn.preprocessing import StandardScaler
    ss = StandardScaler()
    ss.fit(train_input, train_target) # fit해야 transform 작동, 
    train_scaled = ss.transform(train_input)
    test_scaled = ss.transform(test_input)
    
    4. SGD # 분류모델은 클래스 name에 Classifier 붙어 있음
    from sklearn.linear_model import SGDClassifier
    sc = SGDClassifier(loss='log_loss', max_iter=100) # log_loss 방식 활용하는데, 100번 반복해 100개의 그래프 그리겠단 것 >> max_iter: 에포크 몇 번 돌릴거냐는 것
        # loss 기본값: hinge임 | loss, max_iter 값 조정해서 최적 옵션 설정해야 함
    sc.fit(train_scaled, train_target)
    print(sc.score(train_scaled, train_target))
    print(sc.score(test_scaled, test_target))
    
    5. 100번 반복한 것을 다시 이어서 반복하도록 한 것 >> 총 200번 돌린 것
    sc.partial_fit(train_scaled, train_target)
    print(sc.score(train_scaled, train_target))
    print(sc.score(test_scaled, test_target))
        # 근데 무작정 에포크 많이 돌린다고 좋은 건 아님, 최적화 지점 찾아야 함
        # 에포크 반복은 어느 수준 이상 돌리면 일정한 지점 나옴 -> 최적화 지점 있단 것
    
    ```
    

# 4. 트리 알고리즘

- 비선형적 데이터일 때 사용

## 결정 트리

- like 스무고개 → 질문 통해 비슷 군집끼리 분류 가능
    - 선형 회귀와 달리 정규화X도 됨 → 바로 train_input/test_input 넣기
    - 결정트리 목표: 지니 불순도 0에 가깝게 만들기 = 한 질문의 편향 높이는 것 = 데이터 잘 분류하는 것
- **지니 불순도**: 내가 얼마나 한쪽으로 편향된 데이터 갖고 있는지 대한 수치 의미
    
    → 내가 가진 데이터에 얼마나 섞여 있는지 대한 수치
    
    → 편향 강할수록 0 나옴 → 잘 나눈 질문(긍정적)
    
    - 선으로 그릴 수 없는 데이터에 유용, 스무 고개 수많이 반복하는 방식
    
    ```python
    # 결정 트리 모델 활용
    from sklearn.tree import DecisionTreeClassifier
    dt = DecisionTreeClassifier()
    dt.fit(train_scaled, train_target)
    print(dt.score(train_scaled, train_target))
    print(dt.score(test_scaled, test_target))
    ```
    
    - 결정 트리 시각화
        - 질문1: 알콜, 슈가, pH 중 1번째 데이터(슈가)가 -0.222 이상인지 판별
            
            → 이런 식으로 계속 분류
            
    
    ```python
    import matplotlib.pyplot as plt
    from sklearn.tree import plot_tree
    plt.figure(figsize=(10,7))
    plot_tree(dt, max_depth=1)# max_depth=1: 최상단 트리 1개만 보게 깊이 제한
    plt.show()
    ```
    

## 검증 세트

- 훈련 세트 → 검증 세트(훈련 잘 됐나 검증하는 세트) → 테스트 세트(찐최종 검증할 때만 쓰는 세트)
- 예제: 검증 세트 만들기
    
    ```python
    # 데이터 불러오기
    import pandas as pd
    wine = pd.read_csv('https://bit.ly/wine_csv_data')
    
    data = wine[['alcohol', 'sugar', 'pH']]
    target = wine[['class']]
    
    # 훈련-테스트 셋 나누기
    from sklearn.model_selection import train_test_split
    train_input, test_input, train_target, test_target = train_test_split(data, target)
    
    # train 데이터 다시 나누기: sub-val로 나누기 -> sub: 학습용 데이터, val: 학습 검증용 데이터
    sub_input, val_input, sub_target, val_target = train_test_split(train_input, train_target)
        # sub, val 활용해 학습 시키고 -> 최종으로 test로 검증하는 것 -> 교차 검증
    
    # 결정 트리 모델
    from sklearn.tree import DecisionTreeClassifier
    dt = DecisionTreeClassifier()
    dt.fit(sub_input, sub_target)
    print(dt.score(sub_input, sub_target))
    print(dt.score(val_input, val_target))
    # 하이퍼 파라미터 조정하며, 수정 마무리 하면 -> 마지막에서 test 넣어서 score 검증 하는 식
    ```
    
- `cross_validate(dt, train_input, train_target)`: 교차 검증 → 내 데이터 중 1/5 떼서 검증하는 식으로 자동화
    - 검증 세트 계속 돌리면 최적화됨 → 따라서 훈련, 검증, 테스트 3개 서로 랜덤으로 돌아가면서 교차검증하면 과대적합 방지 가능
    
    ```python
    from sklearn.model_selection import cross_validate
    scores = cross_validate(dt, train_input, train_target)
    	# 80%의 데이터 위해 교차검증 위해 적절히 섞어서 알아서 쓰란 것
    print(scores)
    ```
    

### 하이퍼 파라미터 튜닝

- GridSearchCV: dict 형태로 조정 원하는 인자 넣으면 최적의 값 도출해줌
    
    ```python
    from sklearn.model_selection import GridSearchCV
    parms = {'min_impurity_decrease': [0.0001, 0.0002, 0.0003, 0.0004],
            'max_depth': range(1,100, 5)}
        # 내가 적용하고 싶은 파라미터를 dict으로 하는 것을 만들어야 함
        # min_impurity_decrease: 지니 불순도 -> 테스트하려는 수치 리스트 넣기
        # max_depth: 트리의 최대 깊이 설정
    gs = GridSearchCV(dt, parms, n_jobs=-1)
    gs.fit(train_input, train_target)
    gs.best_estimator_ # 어떤 경우에 점수 가장 잘 나왔는지
    ```
    
- 예시: 결정 트리 + 하이퍼 파라미터 튜닝 + 교차 검증
    
    ```python
    # 데이터 불러오기
    import pandas as pd
    wine = pd.read_csv('https://bit.ly/wine_csv_data')
    
    data = wine[['alcohol', 'sugar', 'pH']]
    target = wine['class']
    
    # 셋 구분
    from sklearn.model_selection import train_test_split
    train_input, test_input, train_target, test_target = train_test_split(data, target)
    
    # 결정 트리 알고리즘 선언
    from sklearn.tree import DecisionTreeClassifier
    dt = DecisionTreeClassifier()
    
    # dt 인스턴스한 뒤, 하이퍼 파라미터 찾기
    from sklearn.model_selection import GridSearchCV
    parms = {'max_depth': range(1, 20, 1),
            'min_impurity_decrease': [0.1, 0.01, 0.001, 0.0001, 0.00001]}
    gs = GridSearchCV(dt, parms)
    gs.fit(train_input, train_target)
    best_dt = gs.best_estimator_
    
    # 교차 검증하기
    from sklearn.model_selection import cross_validate
    scores = cross_validate(best_dt, train_input, train_target)
    print('교차 검증 정확도:', scores['test_score'].mean())
    
    # 검증 완료했으니, 최적 모델 학습
    best_dt.fit(train_input, train_target)
    print('훈련 정확도:', best_dt.score(train_input, train_target))
    print('테스트 정확도:', best_dt.score(test_input, test_target))
    
    # 새 정보 예측하기
    new_wine = pd.DataFrame([[5, 1, 10]], columns=['alcohol', 'sugar', 'pH'])
    prediction = best_dt.predict(new_wine)
    if prediction[0]==1:
        print('화이트 와인입니다.')
    else:
        print('레드 와인입니다.')
        
    > 교차 검증 정확도: 0.86843173800874
    	훈련 정확도: 0.9068144499178982
    	테스트 정확도: 0.8473846153846154
    	화이트 와인입니다.
    ```