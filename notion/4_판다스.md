# 4. 판다스

하위 작업: 4.1 핵심 (https://www.notion.so/4-1-1a5f558bed61804e945add94691d4244?pvs=21)
생성 일시: 2025년 2월 24일 오전 9:56
최종 편집 일시: 2025년 2월 27일 오전 9:43

### 기본 설정

- python, pandas 폴더 독립적으로 설정하기
    - 프로젝트마다 필요한 모듈 달라서 → 가상 환경 통해 독립 운영하는 것

```python
1. 가상 환경 생성
python -m venv venv # python -m 모듈명 폴더명

2. 가상 환경 활성화
source venv/Script/

3. 모듈 설치
pip install jupyterlab
pip install numpy
	# NumPy: 숫자 여럿 담긴 집합을 쉽게 다루도록 도와주는 툴
pip install pandas

4. gitignore 만들기
python, Windows, mac OS 검색해서 생성 >> pandas 폴더에 <.gitignore> 폴더 생성해 복붙
```

- 각 프로젝트마다 필요한 라이브러리 리스트 적어주기

```python
pip list : 현재 내 상황에서 내가 설치한 라이브러리 이름, 버전 명시해줌
pip freeze: 설치한 라이브러리 이름/버전을 컴퓨터가 이해하기 좋은 방식으로 보기
pip freeze >> requirements.txt: 현재 설치된 모듈 리스트 저장
pip freeze를 실행한 결과를 requirements.txt에 저장해달란 것
pip install -r requirements.txt: 해당 파일 기준으로 모듈 설치하란 것
```

- 커밋 남기기

```python
git init # 폴더 새로 만들었으니까 init 해야 함
```

---

# 1. Numpy

- 대량의 배열을 편리하게 다루기 위해 사용
    - numpy로 *2: 배열 내 개별 데이터에 접근해 연산해줌 → 효율적으로 연산 가능

```python
import numpy as np # numpy 불러오는데, 이를 np로 지금부터 부르겠단 것
my_array = np.arange(10000) # numpy로 0~9999까지의 1차원 배열 생성
> array([   0,    1,    2, ..., 9997, 9998, 9999], shape=(10000,))
```

- 차원
    - 0차원: 스칼라 → 단일 데이터
    - 1차원: 벡터
    - 2차원: 행렬(matrix)
    - 3차원: 텐서(tenser)
- `.shape`: (x, y): array의 가로-세로 구조 출력
    - 0차원: 1열로 늘어진 형태 → y값은 공란으로 출력됨
- `.dtype`: 데이터 종류 알려줌 → int(정수), flort(소수), object(문자열+숫자)
- `.ndim`: 배열이 몇 차원인지 출력

```python
data.shape
> (2, 3) # 가로 2, 세로 3이란 것
data.dtype
```

## 1.2 ndarray

### 생성하기

- `np.array(list_name)`: list를 array 형태로 변환 → 연산 위해 변수에 할당 필요
    - numpy에서 만들어지는 모든 array는 ndarray에서 만들어짐
    
    ```python
    data1 = [1,2,3,4,5]
    arr1 = np.array(data1)
    arr1
    print(type(arr1))
    # > <class 'numpy.ndarray'>
    ```
    
    - `np.array( [  [list1], [list2] …] )`: 리스트 내 여러 리스트 넣은 고차원 array 생성
    
    ```python
    arr2 = np.array([[1.1, 1.2],[3.3, 4.4]])
    print(arr2)
    > [[1.1 1.2]
     [3.3 4.4]]
    ```
    

### 배열 구조 잡기

- 비어 있는 구조 만들기
    - .zeros(row 개수, columns 개수): 0으로 채워진 배열 생성 → 이후 데이터 값 넣기
    - .empty(row, columns): 0 채우거나 or 무작위 숫자 넣은 빈 행렬 생성
        - value 중 0 값 있을 시, 0에도 의미 생김 → 그럴 때 empty 활용

```python
print(np.zeros(10)) # 0이 10개 있는 배열 만들어줌
print(np.zeros( (5, 5) )) # (5,5) 구조의 0으로 채워진 배열 생성
print(np.empty((2,2,2))) #  (2,2) 2개의 서로 다른 배열 생성
```

- `np.arange(10)`: 0~9 순서 있는 배열 생성

```python
print(np.arange(10)) # 0-9까지의 순서 있는 배열 만들어줌
> [0 1 2 3 4 5 6 7 8 9]
```

- 동일 row 동일 value 구조 잡기

```python
for i in range(4):
    arr[i] = i
print(arr)
> [[0. 0. 0. 0.]
 [1. 1. 1. 1.]
 [2. 2. 2. 2.]
 [3. 3. 3. 3.]]
```

### 자료형

- 선 자료형 변경 >> 후 배열 생성
    - dtype 인자 직접 설정해서 해당 배열을 어떻게 바라볼 것인지 지정 가능

```python
arr1 = np.array([1,2,3], dtype= np.float64) # 소수형 64비트로 설정
arr2 = np.array([1,2,3], dtype= np.int32) # 정수형 32비트로 설정
```

- 선 배열 생성 >> 후 자료형 변경
    - `.astype(np.무엇으로 변환할지)`: 형변환해주는 함수

```python
arr1 = np.array([1,2,3])
float_arr1 = arr1.astype(np.float64) # 형변환해줌
```

- 소수를 정수로 변경할 때: 소수점 이후는 버림 처리 → 정수형으로 강제로 형변환

```python
arr2 = np.array([1.1,2.2,3.3])
int_arr2 = arr2.astype(np.int64) # 기존 소수를 정수형으로 변경
> [1 2 3]
```

- 사칙연산: 동일 위치 데이터끼리 연산 이뤄짐 → `+`, `-`, `/`, `*`, `**`

```python
arr = np.array([[1,2,3], [4,5,6]])
print(arr)
> [[1 2 3]
 [4 5 6]]

print(arr * arr)
> [[ 1  4  9]
 [16 25 36]]

print(arr + arr)
> [[ 2  4  6]
 [ 8 10 12]]
```

- 비교하기: 동일 위치 데이터끼리 비교해 T/F 값 도출 → `>`, `<`, `==` 사용 가능

```python
arr = np.array([[1,2,3], [4,5,6]])
arr2 = np.array([[3,2,1], [1,2,3]])

print(arr > arr2)
> [[False False  True]
 [ True  True  True]]
 
 print(arr == arr2)
 > [[False  True False]
 [False False False]]
```

## 1.3 인덱싱과 슬라이싱

- 인덱싱(색인): ex. `arr[5]`: 5번째 데이터 접근
- 슬라이싱: ex. `arr[2:5]`: 2~4번째 데이터 자르기
    - numpy는 원본 데이터 슬라이싱 방식임

```python
arr = np.arange(10) # 0-9까지 값 넣은 배열 만듦
arr[5] # 5번째 데이터 값 접근
print(arr[2:5]) # 2번째부터 4번째까지 데이터 슬라이싱해 출력

# 배열 슬라이싱 한 뒤 대신 10 넣기
arr[2:5] = 10
print(arr) # 기존 2, 3, 4 대신 10, 10, 10 넣음
```

- 2차원 데이터 색인

```python
# 2차원 데이터 색인하기
arr = np.array([[1,2,3],[4,5,6]])
print(arr) # 2행 3열 형태의 배열 만들어짐
# 5 접근해 출력
print(arr[1,1]) # 1번째 행의, 1번째 열에 접근한단 것
```

- 3차원 데이터 색인

```python
# 3차원 데이터 색인
arr3d = np.array([[[1,2,3], [4,5,6], [7,8,9]],
                [[11,12,13], [14,15,16], [17,18,19]],
                [[11, 22, 33], [44,55,66], [77,88,99]]])
# '22' 찾기
print(arr3d[2, 0, 1]) # 2번째 행의, 0번째 열의, 1번째 데이터 출력

# 슬라이싱
arr3d[:1] # 0~1번째 열까지 자르기
print(arr3d[0, 1:][1:])
	# 0번째 행의 1번째 행부터 끝까지 선택 >> 그중 1번째 행부터 끝까지 선택
print(arr3d[0, 1:, 1:])
	# 0번째 행 접근 >> 그중 1번째 행부터 선택 >> 그중 1번째 열부터 끝까지 선택
```

### bool indexting

- 예제: hong의 데이터만 고르기
    - bool 데이터 생성해 객체 con에 저장 → 불리언 인덱싱 → 조건 맞는 데이터만 추출
    - ex. `con = names == 'hong'`: 불리언 데이터 저장 → `data[con]` 통해 색인
    
    ```python
    1. list 생성
    names = np.array(['hong', 'kim', 'hong', 'kim'])
    data = np.array([['math', 60], ['math', 90], ['engilsh', 70], ['engilsh', 50]])
    
    2. 불리언 데이터 만들기
    names == 'hong'
    
    3. 불리언 인덱싱
    data[names == 'hong']
    	# data 안에 [[true], [fals], [true], [fals]]를 넣어서, true 값만 출력하는 원리
    data[names == 'hong', 1] # hong의 수학, 영어 점수만 출력하기 >> 인덱스 접근한 것
    ```
    
    - 특정 값만 제외하기:
        - `!=`: 해당 값 아닌 것 고르기
        - `~`: 조건 연산해 값 저장 → 결과의 역치 출력
    
    ```python
    print(names != 'hong') # !=: hong이 아닌 이름을 고르기
    print(~(names == 'hong')) # ~: 계산 결과 출력한 뒤, 그 결과의 역을 출력하는 것
    > [False  True False  True]
    ```
    

### fancy indexing

- 특정 row 내가 지정한 순서대로 색인하기
    - ex. `arr[[4, 3]]`: 4번째, 3번째 행 선택
- 음수 인덱스 사용 가능
    - `arr[[-3, -5, -1]]`: 뒤에서 3번째, 5번째, 1번째 행 선택
- 행/열 동시에 선택하기
    
    ```python
    # 특정 데이터 색인
    arr[[1, 5], [2, 3]] # 1번째 행의 2번째 열, 5번째 행의 3번재 열 선택
    # 팬시 색인 활용
    arr[[1, 5]][:, [2, 3]] # 1번째 행의 2~3번째 열, 5번째 행의 2~3번째 열 선택
    ```
    

### 배열 전치/행렬의 곱

- 배열 축 전환: `객체명.T` → 행렬의 곱 하기 위함
- 행렬의 곱: `arr @ arr.T`, `np.dot(arr, arr.T)`
    - AI는 주로 행렬의 곱 연산 수행 → 해당 연산 처리 속도는 AI 성능과 직결

### numpy 함수

- `np.random.standard_normal(size = (x,y))`: 배열 생성 + 정규분포 따르는 랜덤 숫자 넣기
    
    ```python
    samples = np.random.standard_normal(size = (3,3))
    > [[-0.76183269 -1.77561372  0.66842651]
     [ 0.29934745  1.12619107 -0.59061693]
     [ 0.93856184  0.30366721  0.55314052]]
    ```
    
- `np.sqrt(array_name)`: 배열 내 각 데이터 루트 씌우기
    - nan(결측) 값: 루트 씌운 값이 음수일 때 결측 처리 됨
- `np.abs(array_name)`: 절대값으로 바꾸기
- `.isnan(array_name)`: 현재 내 배열 중 nan이 있는지 확인해줌 → true면 nan 값O

---

# 2. Series

## 2.1 Series 데이터 구조

- pandas에서 사용하는 1차원 배열 → index 사용 가능, 음수 인덱싱은X
    - 엑셀에서 1개 열만 쓴 것과 동일한 형태 → 열 옆에 인덱스 번호 매겨져 있듯
    - 데이터 프레임: 1차원 배열(Series)을 합친 것
    - 구조: `객체 = pd.Series(array_name)`
- 데이터 타입 변경하기
    - `pd.Series(array_name, dtype=' ')`
    - 예시
    
    ```python
    s = pd.Series(arr, dtype='int32') # float(소수)로 변경
    ```
    
- 인덱스 다루기
    - .index: 형성된 인덱스 정보 출력해줌
    - `.iloc[ ]`: 문자열 대신
    - 인덱스 숫자 대신 문자 등으로 설정 변경 가능 → 부여한 인덱스로 접근O
    
    ```python
    # 인덱스 설정 변경
    names = pd.Series(['kim', 'lee', 'park'], index = ['a', 'b', 'c'])
    
    # 문자열 인덱스 대신 숫자로 접근하기
    names.iloc[0]
    ```
    
- 함수
    - `names.values`: 데이터 값만 출력
    - `names.shape`: 차원 갯수 알려줌
    - `names.ndim`: 몇 차원인지 알려줌

## 2.2 fancy indexing: `[[ ]]`

```python
# 행 이름 문자열로 바꾼 경우
s[['d', 'a']] # 특정한 요소 하나를 내가 지정한 순서대로 가져올 때 팬시 인덱싱 사용

# 문자열 대신 숫자로 접근하기
s.iloc[[3,1]] # iloc 사용하면, 숫자로도 인덱싱 접근 가능
```

## 2.3 bool indexing

- 원하는 정보만 출력하기 위해 사용
- 예시
    - 특정 조건 충족하는 데이터만 추출
    
    ```python
    s[s == 'banana'] # 바나나인 것만 s 데이터에서 추출하기
    ```
    
    - 3 초과하는 데이터만 추출
    
    ```python
    s = pd.Series([1,2,3,4,5,6])
    s[s > 3]
    ```
    

## 2.4 결측치(NaN) 처리

- 결측값 제외/포함하기
    - `.isnull( )`: nan 값 가진 경우 true 도출 → 활용해 불리언 인덱싱 가능
    - `.isnull( )`: nan 값인 인덱스만 추출하기
        - `.isna( )`: 동일 코드
    - `.notnull( )`: nan이 아닌 인덱스만 추출하기
        - `.notna( )`: 동일 코드
    
    ```python
    # 결측치 포함된 정보 만들기
    s = pd.Series([1, 3, np.nan, 10, 11, np.nan])
    # nan 제외해 추출
    s[s.notna()]
    ```
    

## 2.5 slicing

- 숫자 인덱스인 경우: 초과/미만O
    - ex. `s[1:3]`: 1~2번째까지 자르기
- 문자열 인덱스인 경우: 초과/미만X
    - ex. s[’a’, ‘b’]: a부터 b까지 출력

# 3. Data Frame

## 3.1 Data Frame 구조

- 2차원 데이터 구조 → excel, sheet와 유사
- row(행): 가로, column(열): 세로 → 이런 구조로 이뤄져 있음
- 구조
    - row는 그대로, column에는 문자열 넣음

```python
d = pd.DataFrame([
    [1,2,3],
    [4,5,6],
    [7,8,9]
], columns = ['가', '나', '다'] )

d
>  가나다
	0	1	2	3
	1	4	5	6
	2	7	8	9
```

- dict 활용해 데이터 프레임 만들기
    - key, value 값 인식해 표 그려줌

```python
info = {
    'name': ['김', '이', '박'],
    'age': [10,20,30]
}
pd.DataFrame(info)
```

- 함수
    - `pd.DataFrame(객체명)`
    - `.index`: 가로 행 몇 개 있는지 정보 출력
    - `.columns`: 세로 열의 정보 출력
    - `.values`: 데이터 값(=values) 정보 출력
    - `.dtypes`: columns 별 데이터 타입 정보 출력
    - `.T`: 행/열 전환하기

### row 다루기

- row 인덱스 이름 변경 → 일반적으로 바꿔 사용하진X

```python
info_df.index = list('abc')
```

### columns 다루기

- 특정 columns만 가져오기 → Series로서 가져오는 원리

```python
info_df['name'] # name 칼럼만 가져오기
```

- fancy indexing하기: `[[]]`
    - 내가 지정한 columns을 지정한 순서대로 선택 → 새 형태의 데이터 프레임 생성

```
info_df[['age', 'name']] # age, name 순서대로 해당 칼럼만 가져오기
```

### 파일 접근하기

- excel 파일 불러오기
    - openpyxl 설치하기: 엑셀 파일 여는 모듈
    
    ```python
    source venv/Scripts/activate # 가상환경 활성화한 이후
    pip install openpyxl # openpyxl 라이브러리 설치
    ```
    
    - `.read_excel(파일 위치, 시트 이름)`: 엑셀 파일 불러오기
    - `객체명[ [ '칼럼1', '칼럼2' ] ]`: 일부 칼럼만 추출
    - `객체명.to_excel(파일 위치/파일명)`: 엑셀 파일 저장
    
    ```python
    excel = pd.read_excel('data/DAMF2.xlsx') # 엑셀 파일 불러와 할당
    excel['이름'] # 이름 컬럼에 있는 밸류만 출력
    excel[['이름', 'github']] # 2개 컬럼만 추출
    excel.head() # 상위 일부만 출력 >> 데이터 구조 확인 시 사용
    excel = pd.read_excel('data/DAMF2.xlsx', sheet_name = 'menu')
    	>> none으로 하면 모든 시트 출력
    ```
    
- csv 파일 불러오기
    - csv: 쉼표로 데이터 구분한 파일 형식
    - `pd.read_csv(파일 위치)`: csv 파일 불러오기
    - `객체명[ [ '칼럼1', '칼럼2' ] ]`: 일부 칼럼만 추출
    - `객체명.to_csv(파일 위치/파일명)`: csv 파일 저장
    
    ```python
    df = pd.read_csv('data/DAMF2.csv') # csv 파일 불러오기
    df2 = df[['이름', 'email']] # 일부 칼럼만 추출하기
    df2.to_csv('data/sample.csv') # csv 파일 저장
    ```
    
- seaborn 라이브러리: 파일 편하게 불러오기 가능
    - 설치

```python
source venv/Scripts/activate # 가상 환경 활성화
pip install seaborn # 모듈 설치
```

### 데이터 확인하기

- `.head/tail()`: 앞/뒤 5개 보여줌 → 인자 부여하면 해당 갯수만큼 잘라서 보여줌

```python
df. head(10) # 앞에서 10개 보여줌
```

- `.info()`: 내 데이터의 컬럼 대해 필수적 정보 알려줌
    - Non-Null Coun: 결측값 없는 것의 갯수 센 것 >> 데이터 셋 보면 어떤 컬럼 있는지, 뭐가 빠졌는지 확인하기
    - category: 데이터 범위 좁을 때 연산 빨리 하기 위해 카테고리 사용 → ex. 성별, 등급
        
        ex. 좌석 등급 int로 받으면, 0-9돌려야 해 연산 느려짐 → 1-3까지 카테고리로 축소
        
- `value_counts()`: 값의 분포 확인할 때 사용 → 컬럼의 세부 정보 확인

```python
df['pclass'].value_counts() # 1-3등석 몇명 있는지 데이터 분포 보여줌
> pclass
	3    491
	1    216
	2    184
	Name: count, dtype: int64
```

### 속성(attirbute)

- `.shape`: 가로/세로 갯수 몇개인지
- `.columns`: 컬럼만 출력
- `.values`: 밸류만 출력

### 타입 변환

- `astype( )`: int32, int64, str, category …

```python
df['pclass'].astype('int32').head()
	# int32로 데이터 타입 변경하기 + 앞 5개만 보기
```

### 정렬

- `sort_index()`: 인덱스 기준으로 정렬
    - `asscending=False`: 가장 많은 것부터 내림차순 정렬
- `sort_values()`: 밸류 값 기준으로 정렬 → 문자열일 시 알파벳 순
    - 예시
    
    ```python
    df.sort_values('age').head() # 나이 기준 오름차순 >> 어린이부터 나옴
    df.sort_values('class').head() # 문자열 = 알파벳 순
    ```
    
- 2개 컬럼 기준으로 삼을 시, list 활용
    - 예시: 요금, 나이 기준으로 정렬하기
        - 요금 기준 정렬 → 동점일 때 나이 기준 정렬
        - 리스트별로 ascending 각 설정 가능

```python
df.sort_values(['fare', 'age'], ascending=[False, True]).head(50)
# 요금은 내림차순, 동점일 시 나이 내림차순으로 정렬
```

## 3.2 인덱싱

- `.loc[위치, 컬럼명]`: 초과/미만O
- 인덱스 접근할 때는 Series 형태

```python
df.loc[3, ['class', 'age']] # 3번째 사람의 class와 age 정보 출력
```

## 3.3 슬라이싱

- `.loc[num1:num2]`: 파이썬과 달리 마지막 데이터도 포함 → 초과/미만X
- 컬럼도 슬라이싱 가능
- 슬라이싱 접근할 때는 data frame 형태
    - 예시
    
    ```python
    df['class'] # 대괄호 1개 >> 시리즈로 반환
    df[['class']] # 대괄호 2개 >> 데이터 프레임으로 반환
    
    df.loc[2:5, ['pclass', 'who']] # 2-5번째의 pclass, who 정보만 출력
    df.loc[:3, 'class':'deck'] # 열: 0~3번째까지, 컬럼: class~deck까지
    ```
    

## 3.4 조건 필터링

- 비교연산자: T/F로 뭉쳐진 데이터셋 만들어줌 → 이를 condition에 저장해서 활용할 것

```python
condition = df['who'] == 'man' # df[df['who'] == 'man']과 동일한 코드
df[condition].head() # who에 man만 필터링됨
```

- cond로 필터링한 정보 중, 특정 컬럼 접근하기: `loc[cond, ‘컬럼명’]` 활용
    - 예시:
    
    ```python
    df.loc[condition, 'age'] = 100
    # 필터링한 데이터 중 age 컬럼 접근 >> 일괄적으로 값 100으로 변경
    ```
    
- 예제: 타이타닉 요금 30불 초과 & 여성 필터링

```python
1. 요금 30불 이상 낸 사람 필터링
cond1 = df['fare'] > 30

2. 여자만 고르기
cond2 = df['who'] == 'woman'

3. 2개 정보 연결하기
# 왼/오 조건 모두 만족하는 경우만 필터링 >> 30불 이상 지불한 여자만 고르기
df.loc[cond1 & cond2]
# 2개 조건 중 하나라도 맞는 경우: 여자이거나 or 30불 낸 사람이거나, 둘다거나
df.loc[cond1 | cond2].head()
```

- `iloc()`: 컬럼 이름이 아닌 숫자로 접근할 때 사용 → 1번째 사람의 3번째 데이터 출력
    - 인덱스 접근이므로 초과/미만O

```python
df.iloc[1, 3] # 1번째 사람의 3번째 데이터 출력
df.iloc[1:3, 2:5]
```

- `where()`: 조건이 False인 데이터를 일괄적으로 수정하는 함수

```python
cond = df['fare'] < 30 # 요금 30불 미만인 사람 필터링
df['fare'].where(cond, 1).tail()
# 일괄 변경할 때 사용
```

- `isin()`: 여러 조건 필터링 할 때 사용

```python
cond = df['embarked'].isin(['Q', 'S']) # embarked 안에 Q 혹은 S 있는지
df.loc[cond]
```

### 실습: 팁 조건 필터링

- 자료 불러오기

```python
tips = sns.load_dataset('tips') # seaborn이 기본적으로 가진 csv 파일 불러온 것
```

1. 남성 흡연자 중, 팁 상위 10명

```python
cond1 = tips['sex'] == 'Male' # 남성만 필터링
cond2 = tips['smoker'] == 'Yes' # 흡연자만 필터링

df1 = tips.loc[cond1 & cond2] # 2개 조건 모두 충족하는 데이터만 추출
df1.sort_values('tip', ascending=False).head(10) # 상위 10개 출력
# 동일코드: tips.loc[cond1 & cond2].sort_values('tip', ascending=False).head(10)
```

1. 식사 인원이 5명 이상인 저녁 테이블 & 흡연자가 없는 주말 테이블 → total_bill, tip 출력

```python
1. 식사 인원이 5명 이상인 저녁 테이블
cond1 = tips['size'] >= 5
cond2 = tips['time'] == 'Dinner'
# 동일코드: cond1 = tips['size'] >= 5 & tips['time'] == 'Dinner'

2. 흡연자가 없는 주말 테이블
cond3 = tips['smoker'] == 'No'
cond4 = tips['day'].isin(['Sun', 'Sat'])
	# day 컬럼 안에 Sun, Sat 둘 중 하나 있는 경우만 필터링

total = tips.loc[(cond1 & cond2) & (cond3 & cond4)] # 소괄호 없어도 되긴 함
total

3. total_bill, tip 출력하기
total.loc[:, ['total_bil', 'tip']]
```

---

## 3.5 describe 메소드

- `describe()`: 통계의 평균, 표준편차, 최대/최솟값 등 정보 출력해줌
    - 숫자 데이터만 활용 가능
    - 문자열인 경우: include=’object’ 인자 설정하면 됨
        - top: 가장 많이 등장한 밸류 나타냄 >> freq: top의 단어가 등장한 횟수
    
    ```python
    df.describe(include='object') # 문자열도 포함하겠단 것
    ```
    
- `count()`: 결측값 제외한 컬럼의 갯수 세줌
- `mean()`: 평균
    - numeric_only=True: 문자열 포함된 데이터 프레임의 경우, 숫자 데이터만 뽑아 연산
    
    ```python
    df['age'].mean()
    df.mean(numeric_only=True) # 숫자만 골라서 평균 계산 가능
    ```
    
    - 예제
        - 성인 남성의 요금 평균 구하기
        
        ```python
        cond = df['adult_male'] == True
        df.loc[cond, 'fare'].mean()
        	# 조건 필터링한 데이터 중, fare 컬럼만 선택 >> 이후 평균 계산
        ```
        
        - 요금 30-40 지불한 승객 중 1등석인 사람의 나이 평균
        
        ```python
        cond1 = df['fare'] >= 30
        cond2 = df['fare'] <= 40
        cond3 = df['pclass'] == 1
        
        df.loc[cond1 & cond2 & cond3, 'age'].mean()
        	# 조건 필터링한 데이터 중, age 컬럼만 선택 >> 이후 평균 계산
        ```
        
- `median()`: 중앙값 → 이상치/극단값 있을 때 유용
    - 예시: 타이타닉 나이 평균/중앙값 구하기
    
    ```python
    mean_value = df['age'].mean()
    median_value = df['age'].median()
    print(mean_value, median_vale)
    ```
    
- `sum()`: 총합
    - `cumsum()`: 누적합
    - `cumprod()`: 누적곱
    
    ```python
    # 총합
    df.sum(numeric_only=True) # 데이터 프레임 컬럼별 총합
    df['fare'].sum() # fare만 선택 >> 총합 구하기
    # 누적합
    df['fare'].cumsum()
    # 누적곱
    df['age'].cumprod()
    ```
    
- `var()`: 분산 → 개별 값이 평균으로부터 얼마나 떨어져 있는지
    - 예시: 타이타닉 요금 분산
    
    ```python
    df['fare'].var()
    > np.float64(2469.436845743116)
    ```
    
- `std()`: 표준편차
- `min()`: 최솟값
- `max()`: 최댓값
- `agg([연산자])`: 여러 함수 연산 결과 동시에 볼 수 있음
    
    ```python
    df['age'].agg(['max', 'min', 'count'])
    ```
    
    - 여러 컬럼 선택하고 싶다면: `[[]]` 대괄호 2개로 접근해야 함
    
    ```python
    df[['age', 'fare']].agg(['max', 'min'])
    ```
    
- `quantile()`: 분위 계산해줌

```python
df['age'].quantile(0.1) # 나이 기준 하위 10%
> np.float64(14.0)
df['age'].quantile(0.8) # 나이 기준 상위 20%
> np.float64(41.0)
```

- `unipue()`: 고유 값 확인 가능 → 컬럼에 어떠한 범주 있는지 확인 가능

```python
df['who'].unique() # who 컬럼 범주 값 확인
> array(['man', 'woman', 'child'], dtype=object)
df['who'].nunique() # who 컬럼 범주 몇 개인지
> 3
```

- `mode()`: 최빈값
- `corr()`: 각 컬럼끼리의 상관관계를 -1~1로 표현 → 상관관계 분석
    - 절댓값 높을수록 강한 상관관계 → 1: 정비례 | -1: 반비례

```python
df.corr(numeric_only=True) # 숫자 데이터만 뽑아 상관관계 분석
df.corr(numeric_only=True)['survived'] # 일부 컬럼만 뽑아 상광관계
```

---

## 3.6 결측치

- 결측치 다루기
    - 데이터 확인하기: 결측치 & 결측치 아닌 데이터
    - 데이터 프레임 카피본 만들기 → 카피폰에서 결측치 수정 진행
        - `copy()`: 데이터 프레임 카피
    - 결측치 채우고, 결측 데이터 제거
    - 예시: 결측치 확인하기

```python
df_copy = df.copy() # 데이터 프레임 카피
df_copy.isnull().sum() # 결측치 있는지, 있다면 결측값 몇개 있는지 총합 확인
	# 동일코드: df_copy.isna().sum()
df_copy.notnull().sum() # 결측이 아닌 것 몇개 있는지
```

### 결측 데이터 필터링

- 결측치 포함된 컨디션 생성 → 결측 데이터만 필터링 → 결측 값 일괄 적용하기

```python
cond = df_copy['age'].isnull() # T/F로 이뤄져 있는 시리즈를 컨디션으로 만듦
df_copy.loc[cond, 'age']=30 # 결측값 age 일괄 30 넣은 것
```

- `fillna()`: 결측값으로 자동으로 넣음 → 이후 결측치에 인자 값 일괄 적용
    - 연산만 해줌 → 할당 해줘야 데이터 프레임 수정O

```python
df_copy['age'].fillna(100) # 연산
df_copy['age'] = df_copy['age'].fillna(100) # 할당 >> age 칼럼에 넣어줘야 함
```

- category: 카테고리 범주로 설정 안 된 것 추가X → 하려면 카테고리 범주 수정 필요

```python
df_copy['deck'].value_counts() # deck 컬럼에서 어떤 밸류가 가장 많은지 세줌

df_copy['deck'] = df_copy['deck'].cat.add_categories('Z') # 카테고리에 Z 추가하기
df_copy['deck'].fillna('Z') # 결측치 Z로 일괄 변경
```

- dropna(): 하나라도 결측인 row 모두 제거하는 게 기본값
    - `df_copy.dropna(how='all')`: 전체 데이터 값 결측인 경우만 제거

### 실습

1. 타이나틱 나이 결측 제거

```python
age_mean = df_copy['age'].mean() # 평균 계산
df_copy['age'] = df_copy['age'].fillna(age_mean) # 평균 나이 넣어 결측치 제거
df_copy.info()
# 동일코드: df_copy['age'] = df_copy['age'].fillna(df_copy['age']=age_mean)
```

1. 남자 승객의 결측치는 남자 승객 평균
    
    여자 승객 결측치는 여자 승객 평균
    
    결측치 모두 채우고 모든 사람 평균
    

```python
1. 조건 필터링
cond1 = df_copy['sex'] == 'male'
male_mean = df_copy.loc[cond1, 'age'].mean() # 남자 중 age 컬럼 선택

cond2 = df_copy['sex'] == 'female'
female_mean = df_copy.loc[cond2, 'age'].mean()

2. 결측값 채우기: loc 인덱스 접근 통해 남녀 각 서로 다른 나이 평균 넣기
df_copy.loc[cond1, 'age'] = df_copy.loc[cond1, 'age'].fillna(male_mean)
    # 남자만 고른 데이터 프레임에 결측값 채우기
df_copy.loc[cond2, 'age'] = df_copy.loc[cond2, 'age'].fillna(female_mean)
    # 여자만 고른 데이터 프레임에 결측값 채우기
df_copy
```

## 3.7 컬럼 추가

- 특성 여럿 조합하면 새로운 특성 찾을 수 있음
    - ex. 성별+연령 등 합쳐졌을 때 더 유의미한 분석 이뤄지기도 하듯
- 일괄적으로 같은 값 넣기

```python
df_copy['VIP'] = False # VIP라는 컬럼 추가, 값은 False 넣어 생성
```

- 기존 컬럼 연산한 결과를 새 컬럼에 넣기

```python
# sbsp&parch 합해 family로 만들기
df_copy['family'] = df_copy['sibsp'] + df_copy['parch']

df_copy['f/a'] = round(df_copy['fare'] / df_copy['age'], 2) # 반올림해서 소수 2번째 자리까지 출력하겠단 것
```

- 글자끼리 연산(이어붙이기)해서 컬럼 생성

```python
df_copy['gender'] = df_copy['sex'] + '-' + df_copy['who']
> ex. gender: male-man
```

### 삭제: `drop()`

- 행 삭제
    - 인덱스, range(), list[] 접근 가능
    
    ```python
    df_copy.drop(1) # 1번째 행 지정해 직접 삭제
    df_copy.drop(range(5)) # 0~4번째까지 행 삭제
    df_copy.drop([1,3,5,7,9]) # 1, 3, 5, 7, 9 행 삭제
    ```
    
- 열 삭제: `drop(’컬럼명’, axis=1)` → axis=1: 세로 열 의미
    - `inplace=True`: 원본 데이터 직접 삭제하도록 설정하는 옵션
    
    ```python
    df_copy.drop('VIP', axis=1).head()
    df_copy.drop(['deck', 'VIP', 'alive'], axis=1).head()
    	# 한번에 여러 열 삭제
    ```
    

## 3.8 데이터 타입

- `dtype`: 해당 컬럼의 데이터 타입 확인해줌
    
    ```python
    # who: man, woman, child 3가지로 구성된 문자열 데이터임 >> 이를 카테고리로 만들어보기
    df_copy['who'].value_counts()
    
    # 카테고리로 전환
    df_copy['who'] = df_copy['who'].astype('category')
    df_copy['who'].dtype
    ```
    
- `cat.codes`: 문자열 카테고리 범주에 할당된 숫자를 출력
    - 아이=1, 남자=2, 여자=3 → 이런 식으로 할당돼 있음
    
    ```python
    df_copy['who'].cat.codes
    ```
    
- `cat.rename_categories()`: 카테고리 범주 이름 재설정
    
    ```python
    df_copy['who'] = df_copy['who'].cat.rename_categories(['아이', '남자', '여자'])
    df_copy['who'].value_counts() # who의 데이터 범주와 개수 출력
    ```
    

### datetime

- 주중 or 주말의 데이터만 출력하려 할 때 유용
- `pd.date_range()`: 일정한 간격의 날짜·시간을 생성하는 함수
    - `start='20250101'` → 시작 날짜 (2025년 1월 1일)
    - `periods=df.shape[0]` → 생성할 날짜·시간 개수 (데이터프레임의 행 개수와 동일)
        - 원본 데이터 수정 가능성 있으므로 이런 식으로 설정한 것
    - `freq='15h'` → 15시간 간격으로 생성

```python
1. 사전 작업
df.shape[0] # 데이터의 행/열 갯수 알려줌 >> 타이타닉 탑승자 인원 확인 가능
	> 891			# 튜플은 대괄호 접근 가능해서 이런 식으로 가능

2. 날짜 데이터 생성
dates = pd.date_range('20250101', periods=df.shape[0], freq='15h')
dates # 15시간 간격으로 쪼갠 날짜 리스트 891개 만듦

3. date 컬럼 추가
df_copy['date'] = dates
df_copy.info() # datatime64 타입의 컬럼 생성

4. 
df_copy['date'].dt.year # # 각 행별 date 컬럼 year 출력
df_copy['date'].dt.dayofweek # 요일 표시해줌 >> 0: 월요일, 6: 일요일
```

- 특정 날짜만 필터링 가능
    
    ```python
    # 일요일만 필터링
    cond = df_copy['date'].dt.dayofweek == 6
    df_copy.loc[cond]
    ```
    

### 실습: 자전거

- 데이터 프레임 확인하기

```python
df = pd.read_csv('data/seoul_bicycle.csv')
# 대여일자: object 타입임 >> datetime이 아닌, 문자열로 기입돼 있는 것
	# 판다스가 인식 가능한 datetime 형태로 수정 필요
```

- 대여일자 → datetime 타입으로 변경하기
    - `to_datetime()`: datetime 객체로 변경해줌
        - 뭐가 년/월/일인지 포맷해줘야 함
            - ex. Jan → 1로 변경: %b
                
                01 → 1로 변경: %d
                

```python
# to_datetime(): datetime 객체로 변경해줌
df_copy['대여일자'] = pd.to_datetime(df_copy['대여일자'], format='%b-%d-%Y')
    # 지금 format=%b-%d-%Y 이런 형식으로 데이터 들어 있는데, 이를 파이썬이 인식하는 datatime 형태로 바꾸기
df_copy.info()
```

- 특정 요일만 필터링

```python
cond = df_copy['대여일자'].dt.dayofweek == 3
df_copy.loc[cond]
```

- 운동량 컬럼의 값을 숫자로 바꾸기

```python
pd.to_numeric(df_copy['운동량'])
df_copy.loc[2344] # 해당 행에 \N 이라는 문자열 들어 있어서 숫자 전환 안 되고 오류

# 에러날 때 어떻게 할 지를 인자 설정하기
df_copy['운동량'] = pd.to_numeric(df_copy['운동량'], errors='coerce') # 숫자로 못 바꾸는 값은 nan으로 바꾸도록 설정
df_copy['운동량'].info # 숫자형으로 변경 완료
```

- `cut()`: bins 설정한 구간만큼 구간 생성 → 극단값 없이 데이터 균등할 때 유용
    - 운동량 구간 나누기
        - 직접 구간 범위 설정
        
        ```python
        bins = [0, 100, 200, 300, df_copy['운동량'].max()]
        	# 내가 보고 싶은 값 리스트로 지정, df_copy 운동량의 최댓값도 넣음
        pd.cut(df_copy['운동량'], bins) # 내가 입력한 단위 사이에 운동량 있는지 확인 가능
        # > 0 (0.0, 100.0] : 0번째 운동량 = 0~100 사이에 있단 것
        ```
        
        - 설정한 구간 자동 생성
        
        ```python
        pd.cut(df_copy['운동량'], bins=10) # 최솟값~최댓값 사이 10개 구간으로 나눈 것
        pd.cut(df_copy['운동량'], bins=10).value_counts()
        ```
        
- `qcut()`: 데이터를 분위수(비율) 기준으로 나눔 → 데이터 균등하지 않은 경우 사용
    - `quantile()`:
    
    ```python
    pd.qcut(df_copy['운동량'], q=10) # '운동량'을 10개 분위수 대로 나누기
    pd.qcut(df_copy['운동량'], q=10).value_counts()
    ```
    
    - 표준분포로 표기하기: 이상치 어느 정도 제거하고 싶을 때 유용
    
    ```python
    qcut_bins = [0, 0.2, 0.8, 1] # 0퍼부터 20퍼, 20~80퍼, 80~100퍼 구간으로 나누기
    df_copy['운동량_한글'] = pd.qcut(df_copy['운동량'], qcut_bins, labels=['적음', '보통', '많음'])
        # labels 부여해서, 0퍼~20퍼 사이의 행은 운동량 적은 것 등으로 할당하는 것
    ```
    

## 3.9 그룹화

- 성별 기준으로 생존율 평균 구하기
    - 성별 기준으로 그룹핑 >> 이후 성별 따라 평균 계산

```python
df_copy.groupby('sex').mean(numeric_only=True) # 숫자만 평균 계산
```

- 여러 개의 컬럼 그룹핑
    - 먼저 성별 기준으로 그룹핑 >> 각 성별 내에서 몇등석인지 기준으로 그룹핑

```python
df_copy.groupby(['sex', 'pclass']).mean(numeric_only=True)

# 보고 싶은 정보만 선택하기
df_copy.groupby(['sex', 'pclass'])[['survived', 'age']].mean(numeric_only=True)
    # [[]]: 하면 Series를 df 형태로 보여줌
```

reset_index()

```python
# 그루핑해서 선택한 데이터를 >> 인덱스로 풀기
df_temp = df_copy.groupby(['sex', 'pclass'])[['survived']].mean(numeric_only=True)
df_temp.reset_index()
```

- 여러 연산 동시에 출력: `agg()` 활용

```python
df_copy.groupby(['sex', 'pclass'])[['survived', 'age']].agg(['mean', 'median'])
```

### pivot_table()

- 컬럼 이름을 인덱스로 위치 전환 → 2개 컬럼 연관성 확인 위해 교차분석
    
    ```python
    df_copy.pivot_table(index='who', values = 'survived')
    	# who와 survived 간 연관성 확인 >> 아이/남성/여성 중 생존률 보여줌
    df_copy.pivot_table(index=['who', 'pclass'], values='survived')
    	# who(그중 좌석등급)의 생존률 확인 가능
    ```
    

### concat(df1, df2)

- 데이터 프레임 연결할 때 사용하는 함수
    - 위 아래로 합치기
        - 근데 그냥 concat() 하면 인덱스 중복됨 → 인덱스 새로 지정 필요O
        - `ignore_index=True` : 인덱스 번호 새로 만드는 옵션 → `reset_index`와 동일 기능
    
    ```python
    df = pd.concat([df1, df2], ignore_index=True) # 인덱스 새로 지정해 df1+df2 병합
    ```
    
    - 좌우로 합치기
        - `axis =1`: 왼쪽 세로 컬럼에 df2 합쳐 좌우 병합하는 옵션
    
    ```python
    pd.concat([sales1, sales2], axis=1) # 왼쪽 세로 열에 sales2 붙여서 좌우로 합치기
    ```
    

### merge()

- 데이터 프레임 합치기 → R/SQL inner_join의 역할인 셈
    - 교집합: inner_join → 기본값, 합집합:outer_join, 차집합: left/right_join

```python
# 교집합
pd.merge(df1, df2, how='inner')
    # how=left, right,outer,inner >> 해서 어떻게 집합 만들건지 옵션 줄 수 있음
# 합집합
pd.merge(df1, df2, how='outer')	# 서로 없는 데이터는 결측 처리됨
# 오른쪽 차집합: df2에만 있는 사람만 출력, df1에 없는 건 제외됨
pd.merge(df1, df2, how='right')
```

- 합치려는 df 컬럼명 서로 다르다면?

```python
pd.merge(df1, df2, left_on='이름', right_on='고객명')
# 이름-고객명 컬럼 서로 다름 >> 지정한 2개를 축으로 하나로 병합해달란 것
```

### 실습: 영화-리뷰

- 데이터 불러오기

```python
movies = pd.read_csv('data/movies.csv')
reviews = pd.read_csv('data/reviews.csv')
movies.shape
reviews.shape
# movies, reviews 서로 행/열 다름 >> 이를 교집합/합집합 등으로 합치고 싶을 때 merge 쓰는 것
```

- 2005년에 개봉 영화 리뷰만 보기

```python
1. movies + reviews 2개 df 병합
movie_review = pd.merge(movies, reviews)
	# movies의 첫번째 key인 movieId 기준으로 교집합
		# movies에 있는 영화의 리뷰만 merge됨
2. 2005년 개봉 영화의 리뷰 필터링
cond = movie_review['movieYear'] == 2005 # 2005년 개봉 영화만 필터링
movie_review.loc[cond, 'rating'].mean() # 그중 영화 평점만 접근해 평균 매김
```